# -*- coding: utf-8 -*-
"""Source_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MPS8aWEZWa9DXn0PQHAS2waX-YlCp9s1

# **Data Exploration:**
"""

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import OneHotEncoder


#my file name - Marijuana_Arrests.csv
file_path = 'Marijuana_Arrests.csv'
data = pd.read_csv(file_path)

"""Understanding of Data"""

#Understanding of Data
print("Shape of the data (rows, columns):", data.shape)
print("\nFirst few rows of the dataset:\n", data.head())
print("\nData Types:\n", data.dtypes)

"""Summarization Statistics"""

# Summarization Statistics
# Descriptive statistics for numerical features
print("\nSummary statistics for numerical columns:\n", data.describe())

# Descriptive statistics for categorical features
print("\nSummary statistics for categorical columns:\n", data.describe(include='object'))

"""Visualizing the Data"""

#Visualizing the Data
sns.set(style="whitegrid")

# Arrests by Race and Gender
plt.figure(figsize=(8, 6))
sns.countplot(x='RACE', hue='SEX', data=data)
plt.title('Arrests by Race and Gender')
plt.show()

# Visualizing missing data
plt.figure(figsize=(10, 6))
sns.heatmap(data.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Data Heatmap')
plt.show()

# Distribution of arrest types
plt.figure(figsize=(8, 6))
sns.countplot(data['TYPE'])
plt.title('Distribution of Arrest Types')
plt.show()

# Distribution of age
plt.figure(figsize=(8, 6))
sns.histplot(data['AGE'].dropna(), kde=True, bins=20)
plt.title('Age Distribution of Arrests')
plt.show()

# Visualize Gender Distribution
plt.figure(figsize=(8, 6))
sns.countplot(x='SEX', data=data, palette='coolwarm')
plt.title('Gender Distribution of Arrests')
plt.xlabel('Sex')
plt.ylabel('Count')
plt.show()



# Bar Plot for Offense PSA (Police Service Area)
plt.figure(figsize=(10, 6))
sns.countplot(x='OFFENSE_PSA', data=data, palette='dark')
plt.title('Distribution of Arrests by Police Service Area (PSA)')
plt.xlabel('Police Service Area (PSA)')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()


# Heatmap for Offense District and Year (Distribution)
plt.figure(figsize=(12, 6))
offense_district_year = pd.crosstab(data['OFFENSE_DISTRICT'], data['YEAR'])
sns.heatmap(offense_district_year, cmap='Blues', annot=True, fmt='d')
plt.title('Offense Districts by Year')
plt.xlabel('Year')
plt.ylabel('Offense District')
plt.show()

#  Line Plot for Arrest Trend over Time (Grouped by Year)
plt.figure(figsize=(10, 6))
arrests_per_year = data.groupby('YEAR').size()
sns.lineplot(x=arrests_per_year.index, y=arrests_per_year.values, marker='o', color='b')
plt.title('Trend of Arrests Over the Years')
plt.xlabel('Year')
plt.ylabel('Number of Arrests')
plt.grid(True)
plt.show()

#  Visualize Offense District Distribution
plt.figure(figsize=(10, 6))
sns.countplot(x='OFFENSE_DISTRICT', data=data, palette='muted')
plt.title('Distribution of Arrests by Offense District')
plt.xlabel('Offense District')
plt.ylabel('Count')
plt.show()

#  Pie Chart for Gender Distribution
gender_counts = data['SEX'].value_counts()
plt.figure(figsize=(8, 6))
plt.pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%', colors=['lightblue', 'lightcoral'], startangle=90)
plt.title('Pie Chart of Gender Distribution')
plt.show()

# Visualize Ethnicity Distribution
plt.figure(figsize=(8, 6))
sns.countplot(x='ETHNICITY', data=data, palette='Set2')
plt.title('Ethnicity Distribution of Arrests')
plt.xlabel('Ethnicity')
plt.ylabel('Count')
plt.show()

#  Visualize Yearly Arrest Trend
plt.figure(figsize=(10, 6))
sns.histplot(data['YEAR'], kde=False, bins=10, color='c')
plt.title('Number of Arrests Per Year')
plt.xlabel('Year')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

"""Identify data problems"""

# Checking for missing values
missing_data = data.isnull().sum()
missing_percentage = (missing_data / len(data)) * 100
missing_df = pd.DataFrame({'Missing Values': missing_data, 'Percentage': missing_percentage})
print("Missing Data Overview:")
print(missing_df)

# Checking for duplicate records
duplicates = data.duplicated()
duplicate_count = duplicates.sum()
print(f"\nNumber of duplicate records: {duplicate_count}")

# If there are duplicates we find this
if duplicate_count > 0:
    print("\nDuplicate Records:")
    print(data[duplicates])

# Detecting outliers using boxplots for numerical columns (like AGE, YEAR)
import seaborn as sns
import matplotlib.pyplot as plt

# Outlier detection in AGE
plt.figure(figsize=(8, 6))
sns.boxplot(x=data['AGE'], palette="Set2")
plt.title('Outlier Detection in Age')
plt.show()

# Outlier detection in YEAR
plt.figure(figsize=(8, 6))
sns.boxplot(x=data['YEAR'], palette="Set1")
plt.title('Outlier Detection in Year')
plt.show()

# Statistical summary of the numerical columns
print("\nStatistical Summary of Numerical Columns:")
print(data[['AGE', 'YEAR']].describe())

"""# **Data Preprocessing:**

**Cleaning**

Check for Missing Values
"""

#Check for Missing Values
print("Missing values per column:\n", data.isnull().sum())

#Check for Outliers in Numerical Columns
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns

for col in numerical_columns:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x=data[col])
    plt.title(f'Boxplot of {col} for Outliers')
    plt.show()
    print(f"\n{col} column statistics:\n", data[col].describe())

#Check for Inconsistent Categorical Values
categorical_columns = data.select_dtypes(include=['object']).columns

for col in categorical_columns:
    unique_values = data[col].unique()
    print(f"\nUnique values in '{col}' column:\n", unique_values)

# Data Type Validation need for some part
print("\nData Types:\n", data.dtypes)

"""**Handle missing values (median, mean, and mode).**"""

print("Missing values per column:\n", data.isnull().sum())

# Function to handle missing values by median, mean, or mode
def impute_missing_values(df):
    for col in df.columns:
        if df[col].dtype == 'object':
            # For categorical columns, use mode
            mode_val = df[col].mode()[0]
            df[col].fillna(mode_val, inplace=True)
        else:
            # For numerical columns, use median or mean based on column characteristics
            median_val = df[col].median()
            mean_val = df[col].mean()

            # Replace missing values with median
            df[col].fillna(median_val, inplace=True)


    return df

# Impute missing values
data_imputed = impute_missing_values(data)

# Verify that there are no missing values left
print("Missing values after imputation:\n", data_imputed.isnull().sum())

"""**Remove incorrect/incomplete data**




*   Binning Smoothing by mean/median/boundary
*   Regression
*   Clustering

"""

# Remove Incorrect/Incomplete Data
# Assuming `data_imputed` is the DataFrame after imputation
data_cleaned = data_imputed.drop_duplicates()
data_cleaned = data_cleaned.dropna()

print(f"Data after removing duplicates and ensuring no missing values: {data_cleaned.shape[0]} rows remaining.")

"""Binning Smoothing by mean/median/boundary"""

# Define bins and labels for AGE column
bins = [0, 18, 30, 40, 50, 60, 100]
labels = ['<18', '18-30', '30-40', '40-50', '50-60', '60+']

# Bin the 'AGE' column
data_cleaned['Age_binned'] = pd.cut(data_cleaned['AGE'], bins=bins, labels=labels, include_lowest=True)

# Smoothing by Mean
data_cleaned['AGE_Smooth_Mean'] = data_cleaned.groupby('Age_binned')['AGE'].transform('mean')

# Smoothing by Median
data_cleaned['AGE_Smooth_Median'] = data_cleaned.groupby('Age_binned')['AGE'].transform('median')

# Boundary Smoothing
bin_edges = pd.cut(data_cleaned['AGE'], bins=bins, labels=False, include_lowest=True)
age_min_max = {i: (bins[i], bins[i+1]) for i in range(len(bins)-1)}

def boundary_smooth(val, bin_idx):
    lower_bound, upper_bound = age_min_max[bin_idx]
    return lower_bound if abs(val - lower_bound) < abs(val - upper_bound) else upper_bound

data_cleaned['AGE_Smooth_Boundary'] = data_cleaned.apply(lambda row: boundary_smooth(row['AGE'], bin_edges[row.name]), axis=1)

# Display results
print("\nSample of Mean Smoothing:\n", data_cleaned[['AGE', 'Age_binned', 'AGE_Smooth_Mean']].head())
print("\nSample of Median Smoothing:\n", data_cleaned[['AGE', 'Age_binned', 'AGE_Smooth_Median']].head())
print("\nSample of Boundary Smoothing:\n", data_cleaned[['AGE', 'Age_binned', 'AGE_Smooth_Boundary']].head())

"""**Reduction:**

1. Dimensionality Reduction
"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Select numerical columns for PCA
numerical_cols = ['AGE', 'OFFENSE_BLOCKX', 'OFFENSE_BLOCKY', 'ARREST_BLOCKX', 'ARREST_BLOCKY']  # Example columns
X = data_cleaned[numerical_cols]

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Add PCA components to DataFrame for inspection
data_cleaned['PCA1'] = X_pca[:, 0]
data_cleaned['PCA2'] = X_pca[:, 1]

print("\nPCA Components:\n", data_cleaned[['PCA1', 'PCA2']].head())

"""2. Attribute Subset Selection"""

from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# Define your features and target variable
X = data_cleaned[numerical_cols]  # Features
y = data_cleaned['TYPE']  # Target variable

# Initialize the model
model = LogisticRegression(max_iter=1000)

# Initialize RFE with the model and desired number of features
rfe = RFE(model, n_features_to_select=3)  # Selecting top 3 features
rfe = rfe.fit(X, y)

# Get the feature rankings
ranking = rfe.ranking_
selected_features = X.columns[rfe.support_]

print("\nSelected Features:\n", selected_features)

"""3. Numerosity Reduction"""

from sklearn.cluster import KMeans

# Define the number of clusters
n_clusters = 10

# Initialize and fit KMeans
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
data_cleaned['Cluster'] = kmeans.fit_predict(X_scaled)

# Represent data using cluster centroids
cluster_centers = pd.DataFrame(kmeans.cluster_centers_, columns=numerical_cols)
print("\nCluster Centers:\n", cluster_centers)

"""**Transformation**"""

print("Columns in DataFrame:\n", data_cleaned.columns)
#this used for below

from sklearn.preprocessing import OneHotEncoder, MinMaxScaler

# Assuming `data` is already loaded

# Identify categorical and numerical columns
categorical_columns = ['TYPE', 'OFFENSE_DISTRICT', 'RACE', 'ETHNICITY', 'SEX', 'CATEGORY']
numerical_columns = ['AGE', 'OFFENSE_BLOCKX', 'OFFENSE_BLOCKY', 'DEFENDANT_PSA', 'DEFENDANT_DISTRICT', 'ARREST_BLOCKX', 'ARREST_BLOCKY']

# Handle categorical columns with One-Hot Encoding
ohe = OneHotEncoder(sparse=False, drop='first')
for col in categorical_columns:
    if col in data.columns:
        encoded_data = ohe.fit_transform(data[[col]])
        encoded_df = pd.DataFrame(encoded_data, columns=ohe.get_feature_names_out([col]))
        data = pd.concat([data, encoded_df], axis=1).drop(columns=[col])
    else:
        print(f"Column {col} not found in DataFrame")

# Convert problematic columns to numeric and handle errors
data['AGE'] = pd.to_numeric(data['AGE'], errors='coerce')

# Drop rows with NaN values in 'AGE' if needed
data = data.dropna(subset=['AGE'])

# Initialize MinMaxScaler
scaler = MinMaxScaler()

"""Nomilization"""

# Apply normalization to numerical columns
numerical_columns = [col for col in numerical_columns if col in data.columns and pd.api.types.is_numeric_dtype(data[col])]
if numerical_columns:
    data[numerical_columns] = scaler.fit_transform(data[numerical_columns])
    print("Data after Normalization:\n", data.head())
else:
    print("No valid numerical columns found for normalization.")

"""Discretization"""

# Example of discretizing AGE into bins
bins = [0, 18, 30, 40, 50, 60, 100]
labels = ['<18', '18-30', '30-40', '40-50', '50-60', '60+']
data['AGE_binned'] = pd.cut(data['AGE'], bins=bins, labels=labels, include_lowest=True)

# Example of discretizing OFFENSE_BLOCKX and OFFENSE_BLOCKY into bins
data['OFFENSE_BLOCKX_binned'] = pd.cut(data['OFFENSE_BLOCKX'], bins=5)  # Adjust bin count as needed
data['OFFENSE_BLOCKY_binned'] = pd.cut(data['OFFENSE_BLOCKY'], bins=5)

print(data[['AGE', 'AGE_binned', 'OFFENSE_BLOCKX', 'OFFENSE_BLOCKX_binned', 'OFFENSE_BLOCKY', 'OFFENSE_BLOCKY_binned']].head())

"""concept hierarchy for CATEGORY"""

#we use again our data to use diffrent pandas value

# Load the data
df = pd.read_csv('/content/Marijuana_Arrests.csv')

# Define concept hierarchy for CATEGORY, this is sample sir
category_hierarchy = {
    'Narcotics': ['Narcotics'],
    'Theft': ['Theft', 'Larceny'],
    'Assault': ['Assault', 'Violent Crime'],
    'Property Crime': ['Burglary', 'Theft'],
    'Drug': ['Drug', 'Narcotics'],

}

# Reverse the hierarchy mapping for easy lookup
reverse_category_hierarchy = {v: k for k, values in category_hierarchy.items() for v in values}

# Map detailed categories to broader categories
df['BROAD_CATEGORY'] = df['CATEGORY'].map(lambda x: reverse_category_hierarchy.get(x, 'Other'))

# Define concept hierarchy for TYPE
type_hierarchy = {
    'Possession': 'Drug Offense',
    'Possession with intent to distribute': 'Drug Offense',
    'Assault': 'Violent Crime',
    'Theft': 'Property Crime',
    'Robbery': 'Property Crime',

}

# Map detailed types to broader types
df['BROAD_TYPE'] = df['TYPE'].map(lambda x: type_hierarchy.get(x, 'Other'))

# Display results
df[['CATEGORY', 'BROAD_CATEGORY', 'TYPE', 'BROAD_TYPE']].head()

"""# **Data Mining Part**"""

import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

# Load the data
data = pd.read_csv('/content/Marijuana_Arrests.csv')

# Drop unnecessary columns
columns_to_drop = ['CCN', 'GIS_ID', 'CREATOR', 'CREATED', 'EDITOR', 'EDITED', 'OBJECTID', 'GLOBALID']
data = data.drop(columns=[col for col in columns_to_drop if col in data.columns])

# Encode categorical columns
categorical_columns = ['ADULT_JUVENILE', 'OFFENSE_DISTRICT', 'DEFENDANT_PSA', 'DEFENDANT_DISTRICT', 'RACE', 'ETHNICITY', 'SEX', 'CATEGORY', 'DESCRIPTION']

# Apply Label Encoding to categorical columns
for col in categorical_columns:
    if col in data.columns:
        label_encoder = LabelEncoder()
        data[col] = label_encoder.fit_transform(data[col].astype(str))

# Label encoding for the target variable
label_encoder = LabelEncoder()
data['TYPE_encoded'] = label_encoder.fit_transform(data['TYPE'])

# Drop rows with missing values for correlation calculation
data_clean = data.dropna()

# Check numeric columns after encoding
numeric_columns = data_clean.select_dtypes(include=['number']).columns

# Calculate correlation matrix
corr_matrix = data_clean[numeric_columns].corr()

# Extract correlation with the target variable
corr_with_target = corr_matrix['TYPE_encoded'].sort_values(ascending=False)
print(corr_with_target)

# Plot correlation with the target variable
plt.figure(figsize=(12, 6))
corr_with_target.drop('TYPE_encoded').plot(kind='bar', color='skyblue')
plt.title('Correlation with Target Variable (TYPE)')
plt.xlabel('Features')
plt.ylabel('Correlation')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""Evaluate resulting model/patterns"""

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

# Load the data
data = pd.read_csv('/content/Marijuana_Arrests.csv')

# Drop unnecessary columns
columns_to_drop = ['CCN', 'GIS_ID', 'CREATOR', 'CREATED', 'EDITOR', 'EDITED', 'OBJECTID', 'GLOBALID']
data = data.drop(columns=[col for col in columns_to_drop if col in data.columns])

# Convert DATETIME column to numeric format (e.g., timestamp)
if 'DATETIME' in data.columns:
    data['DATETIME'] = pd.to_datetime(data['DATETIME'], errors='coerce').astype(int) / 10**9  # Convert to seconds since epoch

# Drop rows with missing values
data = data.dropna()

# Encode categorical columns
categorical_columns = ['ADULT_JUVENILE', 'OFFENSE_DISTRICT', 'DEFENDANT_PSA', 'DEFENDANT_DISTRICT', 'RACE', 'ETHNICITY', 'SEX', 'CATEGORY', 'DESCRIPTION', 'ADDRESS']
for col in categorical_columns:
    if col in data.columns:
        label_encoder = LabelEncoder()
        data[col] = label_encoder.fit_transform(data[col].astype(str))

# Label encode the target variable
label_encoder = LabelEncoder()
data['TYPE_encoded'] = label_encoder.fit_transform(data['TYPE'])

# Define features and target variable
X = data.drop(columns=['TYPE', 'TYPE_encoded'])
y = data['TYPE_encoded']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Feature scaling (Standardization)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initialize and train the model
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print("Accuracy Score:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

"""Experiment with Different Parameter Settings"""

from sklearn.model_selection import GridSearchCV

# Define the parameter grid for RandomForestClassifier
param_grid = {
    'n_estimators': [50, 100, 200],  # Number of trees in the forest
    'max_depth': [10, 20, None],     # Maximum depth of each tree
    'min_samples_split': [2, 5, 10], # Minimum number of samples required to split a node
}

# Perform GridSearchCV to find the best parameters
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Best parameters found
print("Best Parameters:", grid_search.best_params_)

# Use the best model from GridSearchCV
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test)

# Evaluate the best model
print("Best Model Accuracy:", accuracy_score(y_test, y_pred_best))
print("\nBest Model Classification Report:\n", classification_report(y_test, y_pred_best))

"""Experiment with Multiple Alternative Methods"""

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier

# Initialize models
log_reg = LogisticRegression(random_state=42)
svm = SVC(random_state=42)
gbc = GradientBoostingClassifier(random_state=42)

# Train and evaluate each model
for model in [log_reg, svm, gbc]:
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"\nModel: {model.__class__.__name__}")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))

"""Improve Preprocessing and Feature Generation"""

# Feature engineering: binning the 'AGE' column
data['AGE_binned'] = pd.cut(data['AGE'], bins=[0, 18, 30, 45, 60, 100], labels=['0-18', '19-30', '31-45', '46-60', '60+'])

# Create interaction features (e.g., combining 'AGE' and 'OFFENSE_DISTRICT')
data['AGE_OFFENSE_DISTRICT'] = data['AGE'].astype(str) + '_' + data['OFFENSE_DISTRICT'].astype(str)

# Label encode the new interaction and binned features
for col in ['AGE_binned', 'AGE_OFFENSE_DISTRICT']:
    if col in data.columns:
        label_encoder = LabelEncoder()
        data[col] = label_encoder.fit_transform(data[col].astype(str))

# Update the feature set
X = data.drop(columns=['TYPE', 'TYPE_encoded', 'AGE'])  # Drop the original 'AGE'
y = data['TYPE_encoded']

# Perform the train-test split again
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardization
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train the best model again with new features
best_model.fit(X_train, y_train)
y_pred_new_features = best_model.predict(X_test)

# Evaluate the model with new features
print("Accuracy with New Features:", accuracy_score(y_test, y_pred_new_features))
print("\nClassification Report with New Features:\n", classification_report(y_test, y_pred_new_features))

"""Increase Amount or Quality of Training Data , we not add this part, because another data column not same this dataset

# **Evaluation and Interpretation:**
â—‹ Results: Present the findings of your analysis.
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import joblib

from sklearn.model_selection import train_test_split


# Load the dataset
data = pd.read_csv('/content/Marijuana_Arrests.csv')

# Drop unnecessary columns
columns_to_drop = ['CCN', 'GIS_ID', 'CREATOR', 'CREATED', 'EDITOR', 'EDITED', 'OBJECTID', 'GLOBALID']
data = data.drop(columns=[col for col in columns_to_drop if col in data.columns])

# Handling missing values by filling with mean for numerical columns and mode for categorical columns
for column in data.select_dtypes(include=['number']).columns:
    data[column].fillna(data[column].mean(), inplace=True)

for column in data.select_dtypes(include=['object']).columns:
    data[column].fillna(data[column].mode()[0], inplace=True)

# Label encoding for categorical variables
label_encoder = LabelEncoder()

# List of categorical columns to encode
categorical_cols = ['TYPE', 'ADULT_JUVENILE', 'OFFENSE_DISTRICT', 'DEFENDANT_DISTRICT', 'RACE', 'SEX', 'CATEGORY']

for col in categorical_cols:
    data[col + '_encoded'] = label_encoder.fit_transform(data[col])

# Select only the relevant columns for the model
X = data[['AGE', 'OFFENSE_BLOCKX', 'OFFENSE_BLOCKY', 'TYPE_encoded', 'RACE_encoded', 'SEX_encoded', 'OFFENSE_DISTRICT_encoded']]
y = data['TYPE_encoded']

# Split the data into training and validation sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling (standardizing numerical features)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""Model Building"""

# Train a RandomForest Classifier
model = RandomForestClassifier(random_state=42)
model.fit(X_train_scaled, y_train)

# Make predictions on the validation set
y_pred = model.predict(X_test_scaled)

# Save the model for future use
joblib.dump(model, 'Police_Arrest_Predictor.pkl')

"""Model Evaluation"""

#Model Evaluation
# Compute performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

# Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Display results
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")
print("Confusion Matrix:")
print(conf_matrix)

"""Cross-Validation and Hyperparameter Tuning"""

# Perform cross-validation to validate model consistency
cross_val_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')
print(f"Cross-Validation Accuracy: {np.mean(cross_val_scores) * 100:.2f}%")

# Hyperparameter tuning using GridSearchCV
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10]
}
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train_scaled, y_train)

# Print the best hyperparameters
print(f"Best Parameters: {grid_search.best_params_}")

# Evaluate the model with the best parameters
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test_scaled)

accuracy_best = accuracy_score(y_test, y_pred_best)
print(f"Best Model Accuracy: {accuracy_best * 100:.2f}%")

"""Visualizing Feature Importance (optional)"""

# Feature importance visualization
importances = model.feature_importances_
feature_names = X.columns
indices = np.argsort(importances)

plt.figure(figsize=(8, 6))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='skyblue')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

# Load your model
import joblib
model = joblib.load('Police_Arrest_Predictor.pkl')

# Feature importance extraction
importances = model.feature_importances_
feature_names = X.columns
indices = np.argsort(importances)

# Plotting Feature Importances

# Horizontal Bar Plot
plt.figure(figsize=(8, 6))
plt.title('Feature Importances (Bar Plot)')
plt.barh(range(len(indices)), importances[indices], color='skyblue')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.tight_layout()
plt.show()

# Horizontal Stacked Bar Plot
plt.figure(figsize=(8, 6))
plt.title('Feature Importances (Stacked Bar Plot)')
plt.barh(feature_names[indices], importances[indices], color='steelblue')
plt.xlabel('Relative Importance')
plt.tight_layout()
plt.show()

# Pie Chart
plt.figure(figsize=(8, 6))
plt.title('Feature Importances (Pie Chart)')
plt.pie(importances[indices], labels=[feature_names[i] for i in indices], autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired.colors)
plt.tight_layout()
plt.show()
